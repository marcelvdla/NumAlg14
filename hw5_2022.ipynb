{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535c17f1-f813-4c48-9bd8-dfc3856c9d07",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06d3e5bf55c941ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Homework set 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bf949-9252-4592-bde1-fab154d5018d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-736ff6bc3e0d0696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected (in the menubar, select Kernel â†’ Restart Kernel and Run All Cells...).\n",
    "\n",
    "Please **submit this Jupyter notebook through Canvas** no later than **Mon Dec. 5, 9:00**. **Submit the notebook file with your answers (as .ipynb file) and a pdf printout. The pdf version can be used by the teachers to provide feedback. A pdf version can be made using the save and export option in the Jupyter Lab file menu.**\n",
    "\n",
    "Homework is in **groups of two**, and you are expected to hand in original work. Work that is copied from another group will not be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f057a-3ad9-4b58-848b-690aab7d7de2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b13bc5ed16bce8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdaace4-9cfe-41cb-b26b-fee6914deec4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fd464f55ba436b1c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Marcel van de Lagemaat - 10886699 <br>\n",
    "Anton Andersen - 14718758 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd967a45-d51e-46ae-a464-402584619799",
   "metadata": {},
   "source": [
    "# The global keyword (helpful info for exercise 2)\n",
    "In exercise 2 you are asked, at some point, to count the number of times a certain function is evaluated. One way of doing this is using a global variable. To change a global variable x from inside a function, the global keyword is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a83140-bdd9-4e11-b53d-6719fe570a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to change a global variable x from inside a function, use the global keyword\n",
    "def foo():\n",
    "    global x\n",
    "    x = x*2\n",
    "    \n",
    "x=4\n",
    "print(\"x before:\", x) \n",
    "foo()\n",
    "print(\"x after:\", x)\n",
    "\n",
    "# verify for yourself that omitting the line \"global x\" produces an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776878a2-b6bd-4573-9dc0-cb3a9d68d553",
   "metadata": {},
   "source": [
    "-----\n",
    "# Exercise 1 (exercise 6.6(d), 2.5 pts)\n",
    "**N.B. This is a pen-and-paper exercise. If you prefer you may upload a separate pdf for this exercise and other pen-and-paper exercises. If you do, don't put both files in a single .zip file, upload them both separately.**\n",
    "\n",
    "Consider the minimization of \n",
    "$$\n",
    "  f(x,y) = x^2 + y^2\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "  g(x,y) = xy^2 -1 = 0 .\n",
    "$$\n",
    "Determine the critical points of the Lagrangian function for this problem and determine whether each is a constrained minimum, a constrained maximum, or neither."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d2058-1b1f-4c27-a5a5-4e1169cf6724",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd779b-f12d-433b-a7a1-48224d0a65b9",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce52e9-1099-4cf9-98b5-e322b7d7673b",
   "metadata": {},
   "source": [
    "## (a) (1 point)\n",
    "The Rosenbrock function is given by\n",
    "\n",
    "$$\n",
    "f(x,y) = 100 (y-x^2)^2 + (1-x)^2\n",
    "$$\n",
    "\n",
    "What is the gradient of $f$? Show that there is exactly one local minimum point and determine this point (N.B. this is a pen-and-paper exercise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87468d82-a8e3-42d8-9515-d531ca4acf72",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b15a0-2bea-4861-a874-a0ac3fdba2f9",
   "metadata": {},
   "source": [
    "## (b) (2 points)\n",
    "Implement the method of steepest descent. Use `scipy.optimize.line_search` as line search method.\n",
    "\n",
    "Test your method on the Rosenbrock function starting from $(x,y) = (0,0)$.\n",
    "Plot the convergence to the minimum: Make a plot of the convergence in the $(x,y)$ plane as well as plot of the norm of the error as a function of the step number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b384d8b4-bf96-4b66-81b7-92dbd95c3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2ef08-a74e-4712-ad5f-acae905b5142",
   "metadata": {},
   "source": [
    "## (c) (1.5 points)\n",
    "\n",
    "Implement the BFGS method for unconstrained optimization, given in Heath chapter 6. Test the correctness of the code using the data in Example 6.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30895688-1c83-41b3-873d-17f37a906796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efef4b-0200-4d10-97a1-f2c9a78d8579",
   "metadata": {},
   "source": [
    "## (d) (1 points)\n",
    "\n",
    "Apply your implementation of the BFGS method to find a local minimum of the Rosenbrock function (see previous exercise). Use starting point $(0,0)$ and do not assume any knowledge of the Hessian when you choose $B_0$.\n",
    "Plot the convergence to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb1e4b-cfe1-4aad-8b50-d7c01be3b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dfc5ce-542f-454c-924e-25de56a6fe7c",
   "metadata": {},
   "source": [
    "## (e) (1 point)\n",
    "How does the convergence compare to that of gradient descent (see\n",
    "previous question)? Let your program count the number of function and gradient evaluations and\n",
    "consider this in your comparison. Implement a stopping criterion in both methods that runs until $||x_k-x^*||_2 < 10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3b21f-2306-40e8-a659-05de85f9ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3e97b-9589-4bed-bf09-d7297719bee1",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
